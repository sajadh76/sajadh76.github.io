<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Sajad Hamzenejadi</title>
    <meta name="author" content="Sajad Hamzenejadi">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/fox_512.ico" type="image/x-icon">
    <link rel="icon" href="images/favicon/fox_512.ico" type="image/x-icon">    
    <link rel="apple-touch-icon" sizes="180x180" href="images/favicon//apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="32x32" href="images/favicon/fox_32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="images/favicon/fox_16.png">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>
</head>

<body>
  <table
    style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Wuyang LI</name>
                  </p>
                  <p class="justified-text">

                    I am a Research Scientist (since Feb 2025) in <a href="https://www.epfl.ch/labs/vita/">Visual
                      Intelligence for Transportation Lab</a> (VITA) at <a href="https://www.epfl.ch/en/">√âcole
                      Polytechnique F√©d√©rale de Lausanne (EPFL)</a>. I feel
                    extremely fortunate to work under the supervision of <a
                      href="https://scholar.google.com/citations?user=UIhXQ64AAAAJ&hl=zh-CN">Prof. Alexandre Alahi</a>.
                  </p>

                  <p class="justified-text">
                    My research journey began with Domain Adaptive Object Detection, which combines two core concepts:
                    <b>perception</b> and <b>domain transfer</b>. Interestingly, this was not only the starting point of
                    my work, but the underlying philosophy has profoundly shaped all of my subsequent research. I firmly
                    believe that breakthroughs in any field rely on two pillars: <b>the depth of perception within that
                      field and the effective transfer of insights from other fields.</b> This philosophy has led my
                    research to span a remarkably broad range of topics, covering four main areas:
                  </p>
                  <ul>
                    <li><strong>AI for Human Life:</strong> Video Generative Model (Current), 3D Spatial Reasoning</li>
                    <li><strong>AI for Autonomous Mobility:</strong> 3D Occupancy Prediction,
                      Open-vocabulary/Cross-domain/Open-set Object Detection</li>
                    <li><strong>AI for Scientific Innovation:</strong> Nanophotonics, Meta Optics, Graph-based Learning,
                      Computational Photography</li>
                    <li><strong>AI for Transforming Medicine:</strong> Brain MRI Analysis, Medical Report Generation, 4D
                      Surgical Simulation</li>
                  </ul>

                  <p class="justified-text">
                    Before this, I worked as a Postdoctoral Researcher (2024‚Äì2025) at the Chinese University of Hong
                    Kong (CUHK) and
                    completed my PhD (2020‚Äì2023) at the City University of Hong Kong (CityUHK) with <span
                      style="color: #b60101;"><strong>Early Graduation</strong></span>, supervised by <a
                      href="https://www.ee.cuhk.edu.hk/~yxyuan/">Prof. Yixuan Yuan</a>. During my PhD, I focused on
                    visual perception in the context of autonomous driving, thoroughly addressing two-dimensional
                    challenges posed by out-of-distribution data and domain shifts. I was also fortunate to have had the
                    opportunity to work with <a
                      href="https://scholar.google.com/citations?user=nTNjqHwAAAAJ&hl=en/">Prof. Bo Han</a>. I
                    completed my undergraduate studies (2016‚Äì2020) at Tianjin University.
                  </p>
                  <p class="justified-text">
                    <span style="color: #b60101;"><strong>
                        I will enter the job market in 2026, seeking Research Scientist positions in video generation,
                        creative AI, and related fields. </strong> I am open to opportunities in any location (e.g., US,
                      Switzerland, China, etc.). If you have any open positions and think I might be a good fit,
                      please feel free to reach out via email (<a href="mailto:wuyang.li@epfl.ch">wuyang.li@epfl.ch</a>)
                      or WeChat (conv-bn-relu).
                    </span>
                  </p>

                  <p style="text-align:center">
                    <a href="mailto:wuyang.li@epfl.ch">E-mail</a> &nbsp/&nbsp
                    <a href="./CV_WuyangLi.pdf">CV</a> &nbsp/&nbsp
                    <!-- <a href="">CV</a>  -->
                    <!-- &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=3Ml_EbAAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/wymanCV">Github</a>
                    &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/wuyang-li-820176208/">LinkedIn</a>

                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/wuyang5.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/wuyang5.png" class="hoverZoomLink"></a>
                  <p style="text-align:center; font-size:12px; color:#666; margin-top:10px; font-style:italic;">
                    Figure 1: Wuyang arrives in France to attend ICCV 2023 and, through a space-time journey, begins
                    searching for job opportunities in 2026.
                  </p>

                </td>
              </tr>
            </tbody>
          </table>
<!-- <body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center; font-weight: bold; font-size: 1.5em;">
                    Sajad Hamzenejadi
                  </p>
                  <p>
                    I'm a PhD student in <strong>Information Systems</strong> at <a href="https://www.unige.ch/en/">University of Geneva (UNIGE)</a>, 
                    previously a research intern at <a href="https://www.bell-labs.com/">Nokia Bell Labs</a> in Paris, France, 
                    and I completed my MSc in <strong>Telecommunications Engineering</strong> at <a href="https://www.polimi.it/">Politecnico di Milano (Polimi)</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:sajad.hamzenejadi@unige.ch">Email</a> &nbsp;/&nbsp;
                    <a href="data/SajadHamzenejadi-CV.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=KvSyQNEAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/sajadh76">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/SH.png">
                    <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/SH.png" class="hoverZoomLink">
                  </a>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body> -->
	
	<!-- 
  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Sajad Hamzenejadi
                </p>
                <p>
		I'm a PhD student in <strong>Information Systems</strong> at <a href="https://www.unige.ch/en/">University of Geneva (UNIGE)</a>,
		previously a research intern at <a href="https://www.bell-labs.com/">Nokia Bell Labs</a> in Paris, France,
		and I completed my MSc in <strong>Telecommunications Engineering</strong> at <a href="https://www.polimi.it/">Politecnico di Milano (Polimi)</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sajad.hamzenejadi@unige.ch">Email</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=KvSyQNEAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/sajadh76">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/SH.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/SH.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table> -->


			<table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>üìë Selected Publication</h2>
                  <p style="font-style: italic; color: #666; margin-top: 10px;">* denotes equal contribution; <span
                      style="background-color: #fff3e0; padding: 2px 8px; border-left: 3px solid #ff9800;">Highlighted</span>
                    papers are representative first-author works</p>
                  <div style="margin-top: 20px; display: flex; gap: 10px; flex-wrap: wrap;">
                    <button class="filter-btn active" onclick="filterPublications('all', this)" style="padding: 10px 20px; border: 2px solid #1772d0; background-color: #1772d0; 
                                   color: white; border-radius: 5px; cursor: pointer; font-size: 14px;">
                      All Papers
                    </button>
                    <button class="filter-btn" onclick="filterPublications('human-life', this)" style="padding: 10px 20px; border: 2px solid #1772d0; background-color: white; 
                                   color: #1772d0; border-radius: 5px; cursor: pointer; font-size: 14px;">
                      AI for Human Life
                    </button>
                    <button class="filter-btn" onclick="filterPublications('autonomous-mobility', this)" style="padding: 10px 20px; border: 2px solid #1772d0; background-color: white; 
                                   color: #1772d0; border-radius: 5px; cursor: pointer; font-size: 14px;">
                      AI for Autonomous Mobility
                    </button>
                    <button class="filter-btn" onclick="filterPublications('scientific-innovation', this)" style="padding: 10px 20px; border: 2px solid #1772d0; background-color: white; 
                                   color: #1772d0; border-radius: 5px; cursor: pointer; font-size: 14px;">
                      AI for Scientific Innovation
                    </button>
                    <button class="filter-btn" onclick="filterPublications('transforming-medicine', this)" style="padding: 10px 20px; border: 2px solid #1772d0; background-color: white; 
                                   color: #1772d0; border-radius: 5px; cursor: pointer; font-size: 14px;">
                      AI for Transforming Medicine
                    </button>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr class="paper-item" data-category="human-life" onmouseout="nerfsuper_stop()"
                onmouseover="nerfsuper_start()" style="background-color: #fff3e0; border-left: 4px solid #ff9800;">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="three">
                    <img src='images/ironman.gif' width="250">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="venue-badge venue-regular">ArXiv 2025</span>
                  <a href="https://stable-video-infinity.github.io/homepage/">
                    <papertitle> Stable Video Infinity: Infinite-Length Video Generation with Error Recycling
                    </papertitle>
                  </a>
                  <br>
                  <strong>Wuyang Li</strong>, Wentao Pan, Po-Chien Luan, Yang Gao, Alexandre Alahi
                  <br>
                  <a href="https://stable-video-infinity.github.io/homepage/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2510.09212">paper</a>
                  /
                  <a href="https://www.youtube.com/watch?v=p71Wp1FuqTw&feature=youtu.be">youtube</a>
                  /
                  <a href="https://github.com/vita-epfl/Stable-Video-Infinity">code</a>
                  <br><strong>Key Words</strong>: Long Video Generation; End-to-end Filming; Human Talking/Dancing
                  Animation
                  <br><strong>Summary</strong>: Stable Video Infinity (SVI) is able to generate ANY-length videos with
                  high temporal consistency, plausible scene transitions, and controllable streaming storylines in ANY
                  domains. SVI incorporates <em><strong>Error-Recycling Fine-Tuning</strong></em>, a new type of
                  efficient training that recycles the Diffusion Transformer (DiT)‚Äôs self-generated errors into
                  supervisory prompts, thereby encouraging DiT to <em>actively correct its own errors</em>.
                </td>
              </tr>
              <tr class="paper-item" data-category="autonomous-mobility" onmouseout="nerfsuper_stop()"
                onmouseover="nerfsuper_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="three">
                    <img src='images/fvg.gif' width="250">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="venue-badge venue-regular">ArXiv 2025</span>
                  <a href="https://vita-epfl.github.io/FVG/">
                    <papertitle>Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in
                      Text-to-Video Diffusion Models
                    </papertitle>
                  </a>
                  <br>
                  Mariam Hassan, Bastien Van Delft, <strong>Wuyang Li</strong>, Alexandre Alahi
                  <br>
                  <a href="https://vita-epfl.github.io/FVG/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2512.16371">paper</a>
                  /
                  <a href="https://github.com/vita-epfl/FVG/tree/main">code (coming)</a>
                  <br><strong>Key Words</strong>: Video Factorization; Text-to-Video Diffusion Models
                  <br><strong>Summary</strong>: We propose Factorized Video Generation (FVG), a simple yet effective
                  pipeline that decomposes text-to-video generation into three stages: reasoning, composition, and
                  temporal synthesis.</em>.
                </td>
              </tr>



              <tr class="paper-item" data-category="autonomous-mobility" onmouseout="nerfsuper_stop()"
                onmouseover="nerfsuper_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="three">
                    <img src='images/rap.gif' width="250">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="venue-badge venue-regular">ArXiv 2025</span>
                  <a href="https://alan-lanfeng.github.io/RAP/">
                    <papertitle>
                      RAP: 3D Rasterization Augmented End-to-End Planning
                    </papertitle>
                  </a>
                  <br>
                  Lan Feng, Yang Gao, √âloi Zablocki, Quanyi Li, <strong>Wuyang Li</strong>, Sichao Liu, Matthieu Cord,
                  Alexandre
                  Alahi
                  <br>
                  <a href="https://alan-lanfeng.github.io/RAP/">project page</a> /
                  <a href="https://arxiv.org/pdf/2510.04333">paper</a> /
                  <a href="https://github.com/vita-epfl/RAP">code</a>
                  <br><strong>Key Words</strong>: End-to-End Planning; 3D Rasterization; Data Scaling
                  <br><strong>Summary</strong>: We propose RAP, a Raster-to-Real feature-space alignment that bridges
                  the
                  sim-to-real gap without requiring pixel-level realism. RAP ranks 1st in the Waymo Open Dataset
                  Vision-based
                  End-to-End Driving Challenge (2025) (UniPlan entry); Waymo Open Dataset Vision-based E2E Driving
                  Leaderboard,
                  NAVSIM v1 navtest, and NAVSIM v2 navhard
                </td>
              </tr>
              <tr class="paper-item" data-category="autonomous-mobility" onmouseout="nerfsuper_stop()"
                onmouseover="nerfsuper_start()" style="background-color: #fff3e0; border-left: 4px solid #ff9800;">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="three">
                    <img src='images/voxdet_teaser.png' width="250">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="venue-badge venue-highlight">NeurIPS 2025 Spotlight</span>
                  <a href="https://vita-epfl.github.io/VoxDet/">
                    <papertitle> VoxDet: Rethinking 3D Semantic Occupancy Prediction as Dense Object Detection
                    </papertitle>
                  </a>
                  <br>
                  <strong>Wuyang Li</strong>, Zhuy Yu, Alexandre Alahi
                  <br>
                  <a href="https://vita-epfl.github.io/VoxDet/">project page</a>
                  /
                  <a href="https://github.com/vita-epfl/VoxDet/blob/main/assets/VoxDet_ArXiv.pdf">paper</a>
                  /

                  <a href="https://github.com/vita-epfl/VoxDet">code</a>

                  <br><strong>Key Words</strong>: 3D Semantic Occupancy Prediction; Dense Object Detection
                  <br><strong>Summary</strong>: 3D semantic occupancy prediction aims to reconstruct the 3D geometry and
                  semantics of the surrounding environment. With dense voxel labels, prior works typically formulate it
                  as a dense segmentation task, independently classifying each voxel without instance-level perception.
                  Differently, VoxDet addresses semantic occupancy prediction with an
                  instance-centric
                  formulation inspired by dense object detection, which uses a VoxNT trick for freely transferring
                  voxel-level class labels to instance-level offset labels.
                </td>
              </tr>

      </tr>
      <tr class="paper-item" data-category="human-life autonomous-mobility" onmouseout="nerfsuper_stop()"
        onmouseover="nerfsuper_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="three">
            <img src='images/seetrek.png' width="250">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="venue-badge venue-regular">NeurIPS 2025</span>
          <a href="https://arxiv.org/pdf/2509.16087">
            <papertitle>
              See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model
            </papertitle>
          </a>
          <br>
          Pengteng Li, Pinhao Song <strong>Wuyang Li</strong>, Weiyu Guo, Huizai Yao, Yijie Xu, Dugang Liu, Hui Xiong
          <br>
          <a href="https://arxiv.org/pdf/2509.16087">paper</a>
          <br><strong>Key Words</strong>: Spatial Understanding; Multimodal Large Language Model
          <br><strong>Summary</strong>: We introduce SEE&TREK, the first training-free prompting framework tailored to
          enhance the spatial understanding of Multimodal Large Language Models (MLLMS) under vision-only constraints.
          While prior efforts have incorporated
          modalities like depth or point clouds to improve spatial reasoning, purely visualspatial understanding remains
          underexplored. SEE&TREK addresses this gap by
          focusing on two core principles: increasing visual diversity and motion reconstruction.
        </td>
      </tr>
      <tr class="paper-item" data-category="scientific-innovation transforming-medicine" onmouseout="nerfsuper_stop()"
        onmouseover="nerfsuper_start()" style="background-color: #fff3e0; border-left: 4px solid #ff9800;">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="three">
            <img src='images/metascope.png' width="250">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="venue-badge venue-highlight">ICCV 2025 Highlight</span>
          <a href="https://cuhk-aim-group.github.io/MetaScope/">
            <papertitle> MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy
            </papertitle>
          </a>
          <br>
          <strong>Wuyang Li*</strong>, Wentao Pan*, Xiaoyuan Liu*, Zhendong Luo, Chenxin Li, Hengyu Liu,
          Din
          Ping Tsai, Mu Ku Chen, Yixuan Yuan
          <br>
          <a href="https://cuhk-aim-group.github.io/MetaScope/">project page</a>
          /
          <a>paper</a>/
          <a href="https://github.com/CUHK-AIM-Group/MetaScope">code (coming)</a>
          <br><strong>Key Words</strong>: Metalens, Computation Photography, Endoscopy, Optical Imaging
          <br><strong>Summary</strong>: Unlike conventional endoscopes limited by millimeter-scale thickness, metalenses
          operate at the micron scale, serving as a promising solution for ultra-miniaturized endoscopy. However,
          metalenses suffer from intensity decay and chromatic aberration. To address this, we developed MetaScope, an
          optics-driven neural network for metalens-based endoscopy, offering a promising pathway for next-generation
          ultra-miniaturized medical imaging devices.
        </td>
      </tr>
      <tr class="paper-item" data-category="human-life" onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="three">
            <img src='images/IR3D-Bench.png' width="250">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="venue-badge venue-regular">NeurIPS 2025</span>
          <a href="https://ir3d-bench.github.io/">
            <papertitle>IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic
              Inverse Rendering
            </papertitle>
          </a>
          <br>
          Parker Liu, Chenxin Li, Zhengxin Li, Yipeng Wu, <strong>Wuyang Li</strong>, Zhiqin Yang,
          Zhenyue Zhang, Yunlong Lin, Sirui Han, Brandon Y. Feng <br>
          <a href="https://ir3d-bench.github.io/">project page</a> /
          <a href="https://ieeexplore.ieee.org/document/10012542">paper</a> /
          <a href="https://github.com/LiuHengyu321/IR3D-Bench">code</a>

          <br><strong>Key Words</strong>: 3D Scene Understanding; Vision-Language Model; Inverse
          Rendering
          <br><strong>Summary</strong>: We propose IR3D-Bench, a benchmark that challenges VLMs to demonstrate real
          scene understanding by actively recreating 3D structures from images using tools. An
          "understanding-by-creating" approach that probes the generative and tool-using capacity of vision-language
          agents (VLAs), moving beyond the descriptive or conversational capacity measured by traditional scene
          understanding benchmarks.
        </td>
      </tr>


      <tr class="paper-item" data-category="scientific-innovation transforming-medicine" onmouseout="nerfsuper_stop()"
        onmouseover="nerfsuper_start()" style="background-color: #fff3e0; border-left: 4px solid #ff9800;">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="three">
            <img src='images/ukan.png' width="250">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://resources.paperdigest.org/2025/09/most-influential-aaai-papers-2025-09-version/"
            style="text-decoration: none;">
            <span class="venue-badge venue-highlight">AAAI 2025 Top-1 most influential paper</span>
          </a>
          <a href="https://yes-u-kan.github.io/">
            <papertitle> U-KAN Makes Strong Backbone for Medical Image Segmentation and
              Generation
            </papertitle>
          </a>
          <br>
          Chenxin Li*, Xinyu Liu*, <strong>Wuyang Li*</strong>, Cheng Wang*, Hengyu Liu,
          Yifan Liu, Zhen Chen, Yixuan Yuan
          <br>
          <a href="https://yes-u-kan.github.io/">project page/</a>
          <a href="https://arxiv.org/pdf/2406.02918">paper/</a>
          <a href="https://github.com/CUHK-AIM-Group/U-KAN">code</a>
          <br><strong>Key Words</strong>: Kolmogorov-Arnold Networks; Medical Image
          Segmentation/Generation; Medical Backbone
          <br><strong>Summary</strong>: We propose the first KAN-based medical backbone,
          U-KAN, which can be seamlessly integrated with existing medical image segmentation
          and generation models to boost their performance with minimal computational
          overhead. <strong>This work has been cited more than 250 times in one year.</strong>
        </td>
      </tr>
          
			
			<!-- <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
<!-- <tr onmouseout="radmesh_stop()" onmouseover="radmesh_start()" bgcolor="#ffffd0">
  <td style="padding:16px;width:20%;vertical-align:middle">
    <div class="one">
      <div class="two" id='radmesh_image'>
        <img src='images/CNN_Medium.gif' width=100%>
      </div>
      <img src='images/CNN_Small.gif' width=100%>
    </div>
    <script type="text/javascript">
      function radmesh_start() {
        document.getElementById('radmesh_image').style.opacity = "1";
      }

      function radmesh_stop() {
        document.getElementById('radmesh_image').style.opacity = "0";
      }
      radmesh_stop()
    </script>
  </td>
  <td style="padding:8px;width:80%;vertical-align:middle">
    <a href="https://half-potato.gitlab.io/rm/">
      <span class="papertitle">Radiance Meshes for Volumetric Reconstruction</span>
    </a>
    <br>
    <a href="https://half-potato.gitlab.io/">Alexander Mai</a>,
    <a href="https://github.com/Shmaug">Trevor Hedstrom</a>,
    <a href="https://grgkopanas.github.io/">George Kopanas</a>, <br>
	<a href="https://mediatech.aalto.fi/~janne/index.php">Janne Kontkanen</a>, 
    <a href="https://jacobsschool.ucsd.edu/faculty/profile?id=253">Falko Kuester</a>,
    <strong>Jonathan T. Barron</strong>
    <br>
    <em>arXiv</em>, 2025
    <br>
    <a href="https://half-potato.gitlab.io/rm/">project page</a>
    /
    <a href="https://arxiv.org/abs/2512.04076">arXiv</a>
    <p></p>
    <p>
	Parameterizing a scene with a Delaunay tetrahedralization and a neural field yields a scene representation that is accurate, fast to render, easy to edit, and backwards-compatible.
    </p>
  </td>
</tr>

<tr onmouseout="nexf_stop()" onmouseover="nexf_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nexf_image'>
					  <img src='images/nexf_after.jpg' width=100%>
					</div>
          <img src='images/nexf_before.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nexf_start() {
            document.getElementById('nexf_image').style.opacity = "1";
          }

          function nexf_stop() {
            document.getElementById('nexf_image').style.opacity = "0";
          }
          nexf_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://m-niemeyer.github.io/nexf/">
          <span class="papertitle">NExF: Learning Neural Exposure Fields for View Synthesis</span>
        </a>
        <br>
        <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>,
        <a href="https://campar.in.tum.de/Main/FabianManhardt">Fabian Manhardt</a>,
        <a href="http://www.lix.polytechnique.fr/Labo/Marie-Julie.RAKOTOSAONA/">Marie-Julie Rakotosaona</a>,
        <a href="https://moechsle.github.io">Michael Oechsle</a>,
        <a href="https://ait.ethz.ch/people/ctsalico">Christina Tsalicoglou</a>,
        <a href="https://scholar.google.com/citations?user=ml3laqEAAAAJ&hl=ja">Keisuke Tateno</a>,
        <strong>Jonathan T. Barron</strong>,
        <a href="https://federicotombari.github.io/">Federico Tombari</a>
        <br>
        <em>NeurIPS</em>, 2025
        <br>
        <a href="https://m-niemeyer.github.io/nexf/">project page</a>
        /
        <a href="https://arxiv.org/abs/2510.08279">arXiv</a>
        <p></p>
        <p>
		Learning a neural field that optimizes exposure for each 3D point enables high-quality 3D-consistent view synthesis despite extreme exposure variation during capture.
        </p>
      </td>
    </tr>
	
    <tr onmouseout="bolt3d_stop()" onmouseover="bolt3d_start()"  bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bolt3d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/bolt3d.mp4" type="video/mp4">
          Your browser does not support the video tag.

          </video></div>
          <img src='images/bolt3d.jpg' width="160">
        </div>
        <script type="text/javascript">
          function bolt3d_start() {
            document.getElementById('bolt3d_image').style.opacity = "1";
          }

          function bolt3d_stop() {
            document.getElementById('bolt3d_image').style.opacity = "0";
          }
          bolt3d_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://szymanowiczs.github.io/bolt3d">
          <span class="papertitle">Bolt3D: Generating 3D Scenes in Seconds</span>
        </a>
        <br>
        <a href="https://szymanowiczs.github.io/">Stanislaw Szymanowicz</a>,
        <a href="https://jasonyzhang.com">Jason Y. Zhang</a>,
        <a href="https://pratulsrinivasan.github.io">Pratul Srinivasan</a>,
        <a href="https://ruiqigao.github.io">Ruiqi Gao</a>,
        <a href="https://github.com/ArthurBrussee">Arthur Brussee</a>,
        <a href="https://holynski.org">Aleksander Holynski</a>,
        <a href="https://ricardomartinbrualla.com">Ricardo Martin-Brualla</a>,
		<strong>Jonathan T. Barron</strong>,
        <a href="https://henzler.github.io">Philipp Henzler</a>
        <br>
        <em>ICCV</em>, 2025
        <br>
        <a href="https://szymanowiczs.github.io/bolt3d">project page</a>
        /
        <a href="https://szymanowiczs.github.io/bolt3d">arXiv</a>
        <p></p>
        <p>
		By training a latent diffusion model to directly output 3D Gaussians we enable fast (~6 seconds on a single GPU) feed-forward 3D scene generation.
        </p>
      </td>
    </tr>

    <tr onmouseout="ever_stop()" onmouseover="ever_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ever_image'>
					  <img src='images/ever_after.png' width=100%>
					</div>
          <img src='images/ever_before.png' width=100%>
        </div>
        <script type="text/javascript">
          function ever_start() {
            document.getElementById('ever_image').style.opacity = "1";
          }

          function ever_stop() {
            document.getElementById('ever_image').style.opacity = "0";
          }
          ever_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://half-potato.gitlab.io/posts/ever/">
			<span class="papertitle">EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis
</span>
        </a>
        <br>
				<a href="https://half-potato.gitlab.io/">Alexander Mai</a>, 
				<a href="https://phogzone.com/">Peter Hedman</a>,
				<a href="https://grgkopanas.github.io/">George Kopanas</a>,
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://scholar.google.com/citations?user=ozNFrecAAAAJ&hl=en">David Futschik</a>,
        <a href="https://xharlie.github.io/">Qiangeng Xu</a>,
        <a href="https://jacobsschool.ucsd.edu/faculty/profile?id=253">Falko Kuester</a>,
				<strong>Jonathan T. Barron</strong>,
        <a href="https://www.zhangyinda.com/">Yinda Zhang</a>
				<br>
        <em>ICCV</em>, 2025 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://half-potato.gitlab.io/posts/ever/">project page</a>
        /
        <a href="https://arxiv.org/abs/2410.01804">arXiv</a>
        <p></p>
        <p>
				Raytracing constant-density ellipsoids yields more accurate and flexible radiance fields than splatting Gaussians, and still runs in real-time.
        </p>
      </td>
    </tr>


    <tr onmouseout="cat4d_stop()" onmouseover="cat4d_start()" bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cat4d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/cat4d.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/cat4d.jpg' width="160">
        </div>
        <script type="text/javascript">
          function cat4d_start() {
            document.getElementById('cat4d_image').style.opacity = "1";
          }

          function cat4d_stop() {
            document.getElementById('cat4d_image').style.opacity = "0";
          }
          cat4d_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://cat-4d.github.io/">
			<span class="papertitle">CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models
</span>
        </a>
        <br>
				<a href="https://www.cs.columbia.edu/~rundi/">Rundi Wu</a>,
				<a href="https://ruiqigao.github.io/">Ruiqi Gao</a>,
				<a href="https://poolio.github.io/">Ben Poole</a>,
				<a href="https://alextrevithick.github.io/">Alex Trevithick</a>,
				<a href="https://www.cs.columbia.edu/~cxz/index.htm/">Changxi Zheng</a>,
				<strong>Jonathan T. Barron</strong>,
				<a href="https://holynski.org/">Aleksander Holynski</a>
        <br>
        <em>CVPR</em>, 2025 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://cat-4d.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2411.18613">arXiv</a>
        <p></p>
        <p>
				An approach for turning a video into a 4D radiance field that can be rendered in real-time. When combined with a text-to-video model, this enables text-to-4D.
        </p>
      </td>
    </tr> -->

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
				  I ‚Äústole‚Äù this guy‚Äôs source code and made it my own! <a href="https://github.com/jonbarron/jonbarron_website">See the original</a>.
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
