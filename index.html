<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Sajad Hamzenejadi</title>
    <meta name="author" content="Sajad Hamzenejadi">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/fox_512.ico" type="image/x-icon">
    <link rel="icon" href="images/favicon/fox_512.ico" type="image/x-icon">    
    <link rel="apple-touch-icon" sizes="180x180" href="images/favicon//apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="32x32" href="images/favicon/fox_32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="images/favicon/fox_16.png">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>
</head>

<body>
  <table
    style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Sajad Hamzenejadi</name>
                  </p>
                  <p class="justified-text">

                    I'm a PhD student in <strong>Information Systems</strong> at <a href="https://www.unige.ch/en/">University of Geneva (UNIGE)</a>, 
                    previously a research intern at <a href="https://www.bell-labs.com/">Nokia Bell Labs</a> in Paris, France, 
                    and I completed my MSc in <strong>Telecommunications Engineering</strong> at <a href="https://www.polimi.it/">Politecnico di Milano (Polimi)</a>.
                  </p>

                  <!-- <p class="justified-text">
                    My research journey began with Domain Adaptive Object Detection, which combines two core concepts:
                    <b>perception</b> and <b>domain transfer</b>. Interestingly, this was not only the starting point of
                    my work, but the underlying philosophy has profoundly shaped all of my subsequent research. I firmly
                    believe that breakthroughs in any field rely on two pillars: <b>the depth of perception within that
                      field and the effective transfer of insights from other fields.</b> This philosophy has led my
                    research to span a remarkably broad range of topics, covering four main areas:
                  </p> -->
                  <!-- <ul>
                    <li><strong>AI for Human Life:</strong> Video Generative Model (Current), 3D Spatial Reasoning</li>
                    <li><strong>AI for Autonomous Mobility:</strong> 3D Occupancy Prediction,
                      Open-vocabulary/Cross-domain/Open-set Object Detection</li>
                    <li><strong>AI for Scientific Innovation:</strong> Nanophotonics, Meta Optics, Graph-based Learning,
                      Computational Photography</li>
                    <li><strong>AI for Transforming Medicine:</strong> Brain MRI Analysis, Medical Report Generation, 4D
                      Surgical Simulation</li>
                  </ul> -->

                  <p style="text-align:center">
                    <a href="mailto:sajad.hamzenejadi@unige.ch">E-mail</a> &nbsp/&nbsp
                    <!-- <a href="./CV_Sajad.pdf">CV</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=KvSyQNEAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/sajadh76">Github</a>
                    &nbsp/&nbsp
                    <a href="https://www.linkedin.com/sajadh76/">LinkedIn</a>

                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/SH.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/SH.png" class="hoverZoomLink"></a>
                  <p style="text-align:center; font-size:12px; color:#666; margin-top:10px; font-style:italic;">
                    Figure 1: Sajad in AI world
                  </p>

                </td>
              </tr>
            </tbody>
          </table>
          
		  <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>ðŸ“‘ Selected Publication</h2>
                  <p style="font-style: italic; color: #666; margin-top: 10px;">* denotes equal contribution; <span
                      style="background-color: #fff3e0; padding: 2px 8px; border-left: 3px solid #ff9800;">Highlighted</span>
                    papers are representative first-author works</p>
                  <div style="margin-top: 20px; display: flex; gap: 10px; flex-wrap: wrap;">
                    <button class="filter-btn active" onclick="filterPublications('all', this)" style="padding: 10px 20px; border: 2px solid #1772d0; background-color: #1772d0; 
                                   color: white; border-radius: 5px; cursor: pointer; font-size: 14px;">
                      All Papers
                    </button>
                    <button class="filter-btn" onclick="filterPublications('human-life', this)" style="padding: 10px 20px; border: 2px solid #1772d0; background-color: white; 
                                   color: #1772d0; border-radius: 5px; cursor: pointer; font-size: 14px;">
                      AI for Human Life
                    </button>
                    <button class="filter-btn" onclick="filterPublications('autonomous-mobility', this)" style="padding: 10px 20px; border: 2px solid #1772d0; background-color: white; 
                                   color: #1772d0; border-radius: 5px; cursor: pointer; font-size: 14px;">
                      AI for Autonomous Mobility
                    </button>
                    <button class="filter-btn" onclick="filterPublications('scientific-innovation', this)" style="padding: 10px 20px; border: 2px solid #1772d0; background-color: white; 
                                   color: #1772d0; border-radius: 5px; cursor: pointer; font-size: 14px;">
                      AI for Scientific Innovation
                    </button>
                    <button class="filter-btn" onclick="filterPublications('transforming-medicine', this)" style="padding: 10px 20px; border: 2px solid #1772d0; background-color: white; 
                                   color: #1772d0; border-radius: 5px; cursor: pointer; font-size: 14px;">
                      AI for Transforming Medicine
                    </button>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr class="paper-item" data-category="human-life" onmouseout="nerfsuper_stop()"
                onmouseover="nerfsuper_start()" style="background-color: #fff3e0; border-left: 4px solid #ff9800;">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="three">
                    <img src='images/ironman.gif' width="250">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="venue-badge venue-regular">ArXiv 2025</span>
                  <a href="https://stable-video-infinity.github.io/homepage/">
                    <papertitle> Stable Video Infinity: Infinite-Length Video Generation with Error Recycling
                    </papertitle>
                  </a>
                  <br>
                  <strong>Wuyang Li</strong>, Wentao Pan, Po-Chien Luan, Yang Gao, Alexandre Alahi
                  <br>
                  <a href="https://stable-video-infinity.github.io/homepage/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2510.09212">paper</a>
                  /
                  <a href="https://www.youtube.com/watch?v=p71Wp1FuqTw&feature=youtu.be">youtube</a>
                  /
                  <a href="https://github.com/vita-epfl/Stable-Video-Infinity">code</a>
                  <br><strong>Key Words</strong>: Long Video Generation; End-to-end Filming; Human Talking/Dancing
                  Animation
                  <br><strong>Summary</strong>: Stable Video Infinity (SVI) is able to generate ANY-length videos with
                  high temporal consistency, plausible scene transitions, and controllable streaming storylines in ANY
                  domains. SVI incorporates <em><strong>Error-Recycling Fine-Tuning</strong></em>, a new type of
                  efficient training that recycles the Diffusion Transformer (DiT)â€™s self-generated errors into
                  supervisory prompts, thereby encouraging DiT to <em>actively correct its own errors</em>.
                </td>
              </tr>
              <tr class="paper-item" data-category="autonomous-mobility" onmouseout="nerfsuper_stop()"
                onmouseover="nerfsuper_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="three">
                    <img src='images/fvg.gif' width="250">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="venue-badge venue-regular">ArXiv 2025</span>
                  <a href="https://vita-epfl.github.io/FVG/">
                    <papertitle>Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in
                      Text-to-Video Diffusion Models
                    </papertitle>
                  </a>
                  <br>
                  Mariam Hassan, Bastien Van Delft, <strong>Wuyang Li</strong>, Alexandre Alahi
                  <br>
                  <a href="https://vita-epfl.github.io/FVG/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2512.16371">paper</a>
                  /
                  <a href="https://github.com/vita-epfl/FVG/tree/main">code (coming)</a>
                  <br><strong>Key Words</strong>: Video Factorization; Text-to-Video Diffusion Models
                  <br><strong>Summary</strong>: We propose Factorized Video Generation (FVG), a simple yet effective
                  pipeline that decomposes text-to-video generation into three stages: reasoning, composition, and
                  temporal synthesis.</em>.
                </td>
              </tr>



              <tr class="paper-item" data-category="autonomous-mobility" onmouseout="nerfsuper_stop()"
                onmouseover="nerfsuper_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="three">
                    <img src='images/rap.gif' width="250">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="venue-badge venue-regular">ArXiv 2025</span>
                  <a href="https://alan-lanfeng.github.io/RAP/">
                    <papertitle>
                      RAP: 3D Rasterization Augmented End-to-End Planning
                    </papertitle>
                  </a>
                  <br>
                  Lan Feng, Yang Gao, Ã‰loi Zablocki, Quanyi Li, <strong>Wuyang Li</strong>, Sichao Liu, Matthieu Cord,
                  Alexandre
                  Alahi
                  <br>
                  <a href="https://alan-lanfeng.github.io/RAP/">project page</a> /
                  <a href="https://arxiv.org/pdf/2510.04333">paper</a> /
                  <a href="https://github.com/vita-epfl/RAP">code</a>
                  <br><strong>Key Words</strong>: End-to-End Planning; 3D Rasterization; Data Scaling
                  <br><strong>Summary</strong>: We propose RAP, a Raster-to-Real feature-space alignment that bridges
                  the
                  sim-to-real gap without requiring pixel-level realism. RAP ranks 1st in the Waymo Open Dataset
                  Vision-based
                  End-to-End Driving Challenge (2025) (UniPlan entry); Waymo Open Dataset Vision-based E2E Driving
                  Leaderboard,
                  NAVSIM v1 navtest, and NAVSIM v2 navhard
                </td>
              </tr>
              <tr class="paper-item" data-category="autonomous-mobility" onmouseout="nerfsuper_stop()"
                onmouseover="nerfsuper_start()" style="background-color: #fff3e0; border-left: 4px solid #ff9800;">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="three">
                    <img src='images/voxdet_teaser.png' width="250">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="venue-badge venue-highlight">NeurIPS 2025 Spotlight</span>
                  <a href="https://vita-epfl.github.io/VoxDet/">
                    <papertitle> VoxDet: Rethinking 3D Semantic Occupancy Prediction as Dense Object Detection
                    </papertitle>
                  </a>
                  <br>
                  <strong>Wuyang Li</strong>, Zhuy Yu, Alexandre Alahi
                  <br>
                  <a href="https://vita-epfl.github.io/VoxDet/">project page</a>
                  /
                  <a href="https://github.com/vita-epfl/VoxDet/blob/main/assets/VoxDet_ArXiv.pdf">paper</a>
                  /

                  <a href="https://github.com/vita-epfl/VoxDet">code</a>

                  <br><strong>Key Words</strong>: 3D Semantic Occupancy Prediction; Dense Object Detection
                  <br><strong>Summary</strong>: 3D semantic occupancy prediction aims to reconstruct the 3D geometry and
                  semantics of the surrounding environment. With dense voxel labels, prior works typically formulate it
                  as a dense segmentation task, independently classifying each voxel without instance-level perception.
                  Differently, VoxDet addresses semantic occupancy prediction with an
                  instance-centric
                  formulation inspired by dense object detection, which uses a VoxNT trick for freely transferring
                  voxel-level class labels to instance-level offset labels.
                </td>
              </tr>

      </tr>
      <tr class="paper-item" data-category="human-life autonomous-mobility" onmouseout="nerfsuper_stop()"
        onmouseover="nerfsuper_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="three">
            <img src='images/seetrek.png' width="250">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="venue-badge venue-regular">NeurIPS 2025</span>
          <a href="https://arxiv.org/pdf/2509.16087">
            <papertitle>
              See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model
            </papertitle>
          </a>
          <br>
          Pengteng Li, Pinhao Song <strong>Wuyang Li</strong>, Weiyu Guo, Huizai Yao, Yijie Xu, Dugang Liu, Hui Xiong
          <br>
          <a href="https://arxiv.org/pdf/2509.16087">paper</a>
          <br><strong>Key Words</strong>: Spatial Understanding; Multimodal Large Language Model
          <br><strong>Summary</strong>: We introduce SEE&TREK, the first training-free prompting framework tailored to
          enhance the spatial understanding of Multimodal Large Language Models (MLLMS) under vision-only constraints.
          While prior efforts have incorporated
          modalities like depth or point clouds to improve spatial reasoning, purely visualspatial understanding remains
          underexplored. SEE&TREK addresses this gap by
          focusing on two core principles: increasing visual diversity and motion reconstruction.
        </td>
      </tr>
      <tr class="paper-item" data-category="scientific-innovation transforming-medicine" onmouseout="nerfsuper_stop()"
        onmouseover="nerfsuper_start()" style="background-color: #fff3e0; border-left: 4px solid #ff9800;">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="three">
            <img src='images/metascope.png' width="250">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="venue-badge venue-highlight">ICCV 2025 Highlight</span>
          <a href="https://cuhk-aim-group.github.io/MetaScope/">
            <papertitle> MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy
            </papertitle>
          </a>
          <br>
          <strong>Wuyang Li*</strong>, Wentao Pan*, Xiaoyuan Liu*, Zhendong Luo, Chenxin Li, Hengyu Liu,
          Din
          Ping Tsai, Mu Ku Chen, Yixuan Yuan
          <br>
          <a href="https://cuhk-aim-group.github.io/MetaScope/">project page</a>
          /
          <a>paper</a>/
          <a href="https://github.com/CUHK-AIM-Group/MetaScope">code (coming)</a>
          <br><strong>Key Words</strong>: Metalens, Computation Photography, Endoscopy, Optical Imaging
          <br><strong>Summary</strong>: Unlike conventional endoscopes limited by millimeter-scale thickness, metalenses
          operate at the micron scale, serving as a promising solution for ultra-miniaturized endoscopy. However,
          metalenses suffer from intensity decay and chromatic aberration. To address this, we developed MetaScope, an
          optics-driven neural network for metalens-based endoscopy, offering a promising pathway for next-generation
          ultra-miniaturized medical imaging devices.
        </td>
      </tr>
      <tr class="paper-item" data-category="human-life" onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="three">
            <img src='images/IR3D-Bench.png' width="250">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="venue-badge venue-regular">NeurIPS 2025</span>
          <a href="https://ir3d-bench.github.io/">
            <papertitle>IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic
              Inverse Rendering
            </papertitle>
          </a>
          <br>
          Parker Liu, Chenxin Li, Zhengxin Li, Yipeng Wu, <strong>Wuyang Li</strong>, Zhiqin Yang,
          Zhenyue Zhang, Yunlong Lin, Sirui Han, Brandon Y. Feng <br>
          <a href="https://ir3d-bench.github.io/">project page</a> /
          <a href="https://ieeexplore.ieee.org/document/10012542">paper</a> /
          <a href="https://github.com/LiuHengyu321/IR3D-Bench">code</a>

          <br><strong>Key Words</strong>: 3D Scene Understanding; Vision-Language Model; Inverse
          Rendering
          <br><strong>Summary</strong>: We propose IR3D-Bench, a benchmark that challenges VLMs to demonstrate real
          scene understanding by actively recreating 3D structures from images using tools. An
          "understanding-by-creating" approach that probes the generative and tool-using capacity of vision-language
          agents (VLAs), moving beyond the descriptive or conversational capacity measured by traditional scene
          understanding benchmarks.
        </td>
      </tr>


      <tr class="paper-item" data-category="scientific-innovation transforming-medicine" onmouseout="nerfsuper_stop()"
        onmouseover="nerfsuper_start()" style="background-color: #fff3e0; border-left: 4px solid #ff9800;">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="three">
            <img src='images/ukan.png' width="250">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://resources.paperdigest.org/2025/09/most-influential-aaai-papers-2025-09-version/"
            style="text-decoration: none;">
            <span class="venue-badge venue-highlight">AAAI 2025 Top-1 most influential paper</span>
          </a>
          <a href="https://yes-u-kan.github.io/">
            <papertitle> U-KAN Makes Strong Backbone for Medical Image Segmentation and
              Generation
            </papertitle>
          </a>
          <br>
          Chenxin Li*, Xinyu Liu*, <strong>Wuyang Li*</strong>, Cheng Wang*, Hengyu Liu,
          Yifan Liu, Zhen Chen, Yixuan Yuan
          <br>
          <a href="https://yes-u-kan.github.io/">project page/</a>
          <a href="https://arxiv.org/pdf/2406.02918">paper/</a>
          <a href="https://github.com/CUHK-AIM-Group/U-KAN">code</a>
          <br><strong>Key Words</strong>: Kolmogorov-Arnold Networks; Medical Image
          Segmentation/Generation; Medical Backbone
          <br><strong>Summary</strong>: We propose the first KAN-based medical backbone,
          U-KAN, which can be seamlessly integrated with existing medical image segmentation
          and generation models to boost their performance with minimal computational
          overhead. <strong>This work has been cited more than 250 times in one year.</strong>
        </td>
      </tr>
	</table>
	
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px 0px;">
          <p style="text-align:right;font-size:small;">
            I stole this guyâ€™s source code! <a href="https://github.com/jonbarron/jonbarron_website">See the original</a>.
          </p>
        </td>
      </tr>
    </tbody>
  </table>
</td></tr></tbody></table>
</body>
</html>
